import discord
from discord.ext import commands
import re
from datetime import datetime
import os
import sys
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import statistics
from urllib.parse import quote
import asyncio

# Force stdout to flush immediately
sys.stdout.reconfigure(line_buffering=True)

# Bot setup
intents = discord.Intents.default()
intents.message_content = True
intents.messages = True

bot = commands.Bot(command_prefix='!', intents=intents)

# Configuration - Using environment variables
MONITORED_CHANNEL_ID = int(os.getenv('MONITORED_CHANNEL_ID', '1417115045573300244'))
PING_ROLE_ID = int(os.getenv('PING_ROLE_ID', '1400527195679490319'))
DISCORD_TOKEN = os.getenv('DISCORD_TOKEN')

# Price extraction patterns
PRICE_PATTERN = r'¬£(\d+\.?\d*)'

def get_driver():
    """Initialize headless Chrome driver"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
    
    driver = webdriver.Chrome(options=chrome_options)
    return driver

def clean_product_name(name):
    """Clean and optimize product name for eBay search"""
    if not name:
        return name
    
    # Remove common words that might narrow search too much
    remove_words = ['[TEST]', 'set of', 'bundle', 'official']
    cleaned = name
    
    for word in remove_words:
        cleaned = re.sub(re.escape(word), '', cleaned, flags=re.IGNORECASE)
    
    # Remove prices
    cleaned = re.sub(r'¬£\d+\.?\d*', '', cleaned)
    
    # Remove extra whitespace
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned

async def scrape_ebay_sold_prices(product_name, max_results=15):
    """Scrape eBay sold listings using Selenium"""
    print(f"üîç Scraping eBay for: '{product_name}'", flush=True)
    
    # Build 3 search variations max
    search_queries = []
    cleaned = clean_product_name(product_name)
    search_queries.append(cleaned)
    
    words = cleaned.split()
    if len(words) >= 3:
        # Try without last word
        search_queries.append(' '.join(words[:-1]))
        # Try without first word
        search_queries.append(' '.join(words[1:]))
    elif len(words) == 2:
        # If only 2 words, just try the original
        pass
    
    # Remove duplicates
    seen = set()
    search_queries = [q for q in search_queries if q and not (q.lower() in seen or seen.add(q.lower()))]
    search_queries = search_queries[:3]  # Max 3 attempts
    
    print(f"   Will try {len(search_queries)} searches: {search_queries}", flush=True)
    
    driver = None
    try:
        # Run in executor to not block Discord bot
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, _scrape_ebay_sync, search_queries, max_results)
        return result
    except Exception as e:
        print(f"   ‚ùå Scraping error: {e}", flush=True)
        return None, None, 0

def _scrape_ebay_sync(search_queries, max_results):
    """Synchronous eBay scraping function"""
    driver = None
    
    try:
        driver = get_driver()
        
        for i, query in enumerate(search_queries, 1):
            try:
                print(f"   [{i}/{len(search_queries)}] Trying: '{query}'", flush=True)
                
                # Build eBay sold listings URL
                search_url = f"https://www.ebay.co.uk/sch/i.html?_nkw={quote(query)}&LH_Sold=1&LH_Complete=1&LH_ItemCondition=1000"
                
                driver.get(search_url)
                
                # Wait a moment for page to load
                import time
                time.sleep(3)
                
                # Try multiple selectors to find items
                items = []
                try:
                    # Try main selector
                    items = driver.find_elements(By.CSS_SELECTOR, ".s-item")
                except:
                    pass
                
                if not items:
                    try:
                        # Try alternative selector
                        items = driver.find_elements(By.CSS_SELECTOR, "li.s-item")
                    except:
                        pass
                
                if not items:
                    try:
                        # Try another alternative
                        items = driver.find_elements(By.XPATH, "//li[contains(@class, 's-item')]")
                    except:
                        pass
                
                if len(items) <= 1:
                    print(f"      ‚ùå No sold items found (found {len(items)} items)", flush=True)
                    
                    # Debug: Print page source snippet
                    try:
                        page_text = driver.page_source[:500]
                        print(f"      Debug: Page starts with: {page_text[:100]}...", flush=True)
                    except:
                        pass
                    
                    continue
                
                print(f"      ‚úì Found {len(items)-1} items on page", flush=True)
                
                # Extract prices - try multiple price selectors
                sold_prices = []
                for idx, item in enumerate(items[1:max_results+1]):  # Skip first placeholder item
                    try:
                        price_text = None
                        
                        # Try different price selectors
                        try:
                            price_elem = item.find_element(By.CSS_SELECTOR, ".s-item__price")
                            price_text = price_elem.text
                        except:
                            try:
                                price_elem = item.find_element(By.XPATH, ".//span[contains(@class, 's-item__price')]")
                                price_text = price_elem.text
                            except:
                                try:
                                    # Try getting any price-like text
                                    price_elem = item.find_element(By.XPATH, ".//span[contains(text(), '¬£')]")
                                    price_text = price_elem.text
                                except:
                                    continue
                        
                        if not price_text:
                            continue
                        
                        # Extract price using regex
                        matches = re.findall(r'¬£([\d,]+\.?\d*)', price_text)
                        if matches:
                            # Clean price (remove commas)
                            price = float(matches[0].replace(',', ''))
                            if 10 < price < 10000:  # Sanity check
                                sold_prices.append(price)
                                
                    except Exception as e:
                        continue
                
                print(f"      Extracted {len(sold_prices)} prices from items", flush=True)
                
                if sold_prices:
                    print(f"      ‚úÖ SUCCESS! Got {len(sold_prices)} valid prices", flush=True)
                    print(f"      Sample prices: {sold_prices[:5]}", flush=True)
                    
                    # Calculate statistics
                    avg_price = statistics.mean(sold_prices)
                    median_price = statistics.median(sold_prices)
                    min_price = min(sold_prices)
                    max_price = max(sold_prices)
                    
                    return {
                        'average': avg_price,
                        'median': median_price,
                        'min': min_price,
                        'max': max_price,
                        'count': len(sold_prices),
                        'prices': sold_prices,
                        'query_used': query
                    }, median_price, len(sold_prices)
                else:
                    print(f"      ‚ö†Ô∏è Items found but couldn't extract valid prices", flush=True)
                    
            except Exception as e:
                print(f"      üí• Error: {e}", flush=True)
                import traceback
                traceback.print_exc()
                continue
        
        print(f"   ‚úó No results from any search variation", flush=True)
        return None, None, 0
        
    finally:
        if driver:
            driver.quit()

def extract_product_info(embed):
    """Extract product name and price from embed"""
    product_name = None
    buy_price = None
    
    # Get product name from title
    if embed.title:
        product_name = clean_product_name(embed.title)
    
    # Extract buy price from fields
    for field in embed.fields:
        text = f"{field.name} {field.value}"
        
        # Look for price
        if 'price' in field.name.lower() or '¬£' in text:
            matches = re.findall(PRICE_PATTERN, text)
            if matches and not buy_price:
                buy_price = float(matches[0])
    
    # Check description for price if not found
    if not buy_price and embed.description:
        matches = re.findall(PRICE_PATTERN, embed.description)
        if matches:
            buy_price = float(matches[0])
    
    return product_name, buy_price

async def create_alert_embed(original_embed, source_message):
    """Create a formatted alert embed with eBay resell data"""
    
    # Extract product info
    product_name, buy_price = extract_product_info(original_embed)
    
    if not product_name:
        product_name = "Unknown Product"
    
    if not buy_price:
        buy_price = 0
    
    # Scrape eBay data
    ebay_data, resell_price, sold_count = await scrape_ebay_sold_prices(product_name)
    
    # Determine actual cost basis
    actual_cost = buy_price
    
    # Create new embed
    if ebay_data and resell_price and resell_price > actual_cost:
        profit = resell_price - actual_cost
        profit_percent = (profit / actual_cost) * 100 if actual_cost > 0 else 0
        
        # Color based on profit
        if profit > 50:
            color = discord.Color.gold()  # Excellent
        elif profit > 20:
            color = discord.Color.green()  # Good
        else:
            color = discord.Color.blue()  # Okay
    else:
        color = discord.Color.orange()  # No data or not profitable
        profit = 0
        profit_percent = 0
    
    alert = discord.Embed(
        color=color,
        timestamp=datetime.utcnow()
    )
    
    # Title
    alert.title = f"üí∞ {product_name}"
    
    # PROFIT AT THE TOP (most important for mobile users)
    if ebay_data and profit > 0:
        profit_emoji = "üü¢" if profit > 20 else "üü°" if profit > 10 else "üîµ"
        profit_text = f"{profit_emoji} **¬£{profit:.2f}** profit ({profit_percent:.1f}%)\n"
        profit_text += f"*Based on {sold_count} recent eBay sales*"
        
        alert.add_field(
            name="üéØ ESTIMATED PROFIT",
            value=profit_text,
            inline=False
        )
    elif ebay_data and sold_count > 0:
        alert.add_field(
            name="‚ö†Ô∏è LOW/NO PROFIT",
            value=f"Recent eBay sales show minimal profit potential\nMedian sold: ¬£{ebay_data['median']:.2f} vs Cost: ¬£{actual_cost:.2f}",
            inline=False
        )
    else:
        alert.add_field(
            name="‚ùì NO EBAY SALES DATA",
            value="‚ö†Ô∏è **Could not find recent sold listings**\n\nThis could mean:\n‚Ä¢ New/rare product\n‚Ä¢ Product name needs refinement\n‚Ä¢ Low demand item\n\n**Manual research required before buying!**",
            inline=False
        )
    
    # Price breakdown
    price_info = []
    price_info.append(f"üè∑Ô∏è **Alert Buy Price:** ¬£{buy_price:.2f}")
    
    if ebay_data:
        price_info.append(f"üìä **eBay Median Sold:** ¬£{ebay_data['median']:.2f}")
        price_info.append(f"üìà **eBay Average Sold:** ¬£{ebay_data['average']:.2f}")
        price_info.append(f"üíµ **Sold Price Range:** ¬£{ebay_data['min']:.2f} - ¬£{ebay_data['max']:.2f}")
        if ebay_data.get('query_used') and ebay_data['query_used'] != product_name:
            price_info.append(f"üîç *Search used: \"{ebay_data['query_used']}\"*")
    else:
        price_info.append(f"‚ùå **Sold Data:** No recent sales found")
    
    alert.add_field(
        name="üìä Price Analysis",
        value="\n".join(price_info),
        inline=False
    )
    
    # Product details from original embed
    details = []
    for field in original_embed.fields:
        field_name_lower = field.name.lower()
        if 'status' in field_name_lower or 'stock' in field_name_lower:
            details.append(f"**{field.name}:** {field.value}")
    
    if details:
        alert.add_field(
            name="üì¶ Product Info",
            value="\n".join(details),
            inline=False
        )
    
    # Links section
    links = []
    for field in original_embed.fields:
        if 'link' in field.name.lower():
            # Extract URLs from the field value
            urls = re.findall(r'https?://[^\s\]]+', field.value)
            if urls:
                links.extend(urls)
            elif 'http' in field.value:
                links.append(field.value)
    
    # Add eBay search link
    search_url = f"https://www.ebay.co.uk/sch/i.html?_nkw={quote(product_name)}&LH_Sold=1&LH_Complete=1"
    links.append(f"[üîç eBay Sold Listings]({search_url})")
    
    if links:
        alert.add_field(
            name="üîó Links",
            value="\n".join(links[:6]),
            inline=False
        )
    
    # Add thumbnail if original has one
    if original_embed.thumbnail:
        alert.set_thumbnail(url=original_embed.thumbnail.url)
    
    # Add image if original has one
    if original_embed.image:
        alert.set_image(url=original_embed.image.url)
    
    # Footer with source
    source_text = original_embed.author.name if original_embed.author else 'Unknown'
    if ebay_data:
        source_text += f" | {sold_count} sales analyzed"
    
    alert.set_footer(text=source_text)
    
    return alert, profit, sold_count

@bot.event
async def on_ready():
    print(f'{bot.user} is now monitoring for deals!', flush=True)
    print(f'Watching channel ID: {MONITORED_CHANNEL_ID}', flush=True)
    print(f'Ping role ID: {PING_ROLE_ID}', flush=True)
    print('ü§ñ Using Selenium web scraper (no rate limits!)', flush=True)
    print('Bot is ready and waiting for embeds...', flush=True)

@bot.event
async def on_message(message):
    # Debug: Log ALL messages in monitored channel
    if message.channel.id == MONITORED_CHANNEL_ID:
        print(f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ", flush=True)
        print(f"üì® Message received in monitored channel!", flush=True)
        print(f"   Author: {message.author} (ID: {message.author.id})", flush=True)
        print(f"   Has embeds: {len(message.embeds)}", flush=True)
        print(f"   Is bot: {message.author.bot}", flush=True)
        print(f"   Is me: {message.author == bot.user}", flush=True)
    
    # Only monitor the specific channel
    if message.channel.id != MONITORED_CHANNEL_ID:
        return
    
    # Ignore ONLY our own messages (not all bots)
    if message.author == bot.user:
        print("‚è≠Ô∏è  Ignoring my own message", flush=True)
        return
    
    # Check if message has embeds
    if not message.embeds:
        print("‚è≠Ô∏è  No embeds found in message", flush=True)
        return
    
    print(f"‚úÖ Processing {len(message.embeds)} embed(s) from {message.author}!", flush=True)
    print(f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ", flush=True)
    
    # Process each embed
    for embed in message.embeds:
        try:
            # Create alert embed with eBay data
            alert_embed, profit, sold_count = await create_alert_embed(embed, message)
            
            # Get role to ping
            role = message.guild.get_role(PING_ROLE_ID)
            
            # Determine alert level based on profit AND data availability
            if sold_count == 0:
                alert_text = "‚ö†Ô∏è **NO SALES DATA - Research Required!**"
            elif profit > 50:
                alert_text = "üî• **HIGH PROFIT DEAL!** üî•"
            elif profit > 20:
                alert_text = "üö® **New Deal Alert!**"
            elif profit > 0:
                alert_text = "üíº **Deal Detected**"
            else:
                alert_text = "‚ÑπÔ∏è **Product Alert** (Low/No Profit)"
            
            # Send notification
            if role:
                await message.channel.send(
                    content=f"{role.mention} {alert_text}",
                    embed=alert_embed
                )
            else:
                await message.channel.send(
                    content=alert_text,
                    embed=alert_embed
                )
                print(f"Warning: Role {PING_ROLE_ID} not found", flush=True)
        
        except Exception as e:
            print(f"Error processing embed: {e}", flush=True)
            import traceback
            traceback.print_exc()
            continue

# Health check endpoint for Railway
@bot.event
async def on_connect():
    print("Bot connected to Discord!", flush=True)

# Run the bot
if __name__ == "__main__":
    if not DISCORD_TOKEN:
        print("ERROR: DISCORD_TOKEN environment variable not set!")
        exit(1)
    
    bot.run(DISCORD_TOKEN)
